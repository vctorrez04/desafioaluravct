# Challenge-Esencia-del-cliente 游늵游뱋

Para este segundo proyecto del bootcamp de Data Science, se desarrollo un an치lisis de datos bastante completo. Este proyecto se enfoca en comprender mejor a los clientes mediante el an치lisis de datos y t칠cnicas avanzadas de clustering. A trav칠s de la implementaci칩n de diversas t칠cnicas de machine learning y reducci칩n de dimensionalidad, el objetivo es identificar patrones y segmentar a los clientes en diferentes grupos, permitiendo a las empresas personalizar sus estrategias de marketing y mejorar la experiencia del cliente.

Los datos utilizados para este proyecto fueron extra칤dos del conjunto de datos disponible en [Kaggle](https://www.kaggle.com/datasets/ramjasmaurya/medias-cost-prediction-in-foodmart), que proporciona informaci칩n detallada sobre los costos y ventas de productos en Foodmart. Este conjunto de datos nos permiti칩 realizar un an치lisis profundo y obtener insights valiosos sobre el comportamiento de los clientes.

El desarrollo y la ejecuci칩n de todo el proyecto se realizaron en Google Colab, una herramienta potente y accesible para el an치lisis de datos y el desarrollo de modelos de machine learning. Google Colab nos permiti칩 aprovechar sus recursos computacionales y colaborar de manera eficiente durante todo el proceso.

Durante este proyecto, se utilizaron varias metodolog칤as y herramientas de Python, incluyendo K-Means, PCA (An치lisis de Componentes Principales) y m칠tricas de evaluaci칩n como Silhouette, Davies-Bouldin y Calinski-Harabasz. Estas t칠cnicas nos permitieron obtener insights valiosos sobre los datos de los clientes y agruparlos en cl칰steres bien definidos.

La importancia de este proyecto radica en su capacidad para transformar datos en informaci칩n accionable. Al identificar y comprender los diferentes segmentos de clientes, las empresas pueden desarrollar estrategias de marketing m치s efectivas, mejorar la personalizaci칩n de sus servicios y, en 칰ltima instancia, aumentar la satisfacci칩n y lealtad de sus clientes.

Este proyecto es una contribuci칩n con todo el amor del mundo para aquellos que buscan formarse en el fascinante 치mbito de la Ciencia de Datos. Espero que mi trabajo pueda servir como una gu칤a y recurso valioso para cualquier persona interesada en mejorar sus habilidades y conocimientos en esta 치rea. #aluraChallengeEsenciaDelCliente



## Pasos que se siguieron para el desarrollo del Challenge 

### 1. Configuraci칩n del Ambiente

Para desarrollar este proyecto, trabajamos en Google Colab. Primero, creamos una cuenta en Gmail si no se tiene. Luego, accedemos a Google Colab y creamos un nuevo Notebook, nombr치ndolo como deseemos (ej. "La esencia del cliente 1"). Conectamos el notebook a Google Drive.

Descargamos el dataset desde las URLs proporcionadas, creamos un directorio en Google Drive y subimos el dataset all칤. Con esto, estamos listos para avanzar a la siguiente etapa.

### 2. Obtenci칩n de los datos 
* Cargamos los archivos almacenados en Google Drive utilizando la biblioteca pandas.

* Traducimos el dataset del ingl칠s al espa침ol para una mejor comprensi칩n. Utilizamos diccionarios de traducci칩n proporcionados en un archivo de Python.

* Exportamos el dataset traducido en formato .csv y lo almacenamos en nuestro directorio de Google Drive para su uso en la siguiente parte del desaf칤o.


### 3. Exploraci칩n de los datos 

La exploraci칩n visual de datos permite identificar caracter칤sticas importantes como valores at칤picos, distribuciones, correlaciones y agrupaciones, que pueden no ser evidentes al examinar solo los n칰meros.

* Utilizamos Matplotlib y Seaborn para generar diversos gr치ficos que nos ayuden a entender mejor los datos.

* Por ejemplo, un histograma puede mostrar la distribuci칩n de ingresos anuales de los clientes. Seleccionamos y visualizamos diferentes variables seg칰n lo consideremos pertinente.

* Registramos nuestras observaciones e hip칩tesis en una celda de texto del notebook a medida que generamos los gr치ficos.

Tip: Algunas variables de inter칠s para el an치lisis visual incluyen Escolaridad, Ocupaci칩n, Miembro, G칠nero, Estado Civil, N칰mero de Hijos, Ingresos anuales, Categor칤a de alimentos y Tipo.

### 4. Preprocesamiento y Obtenci칩n de Features

Codificamos las variables categ칩ricas para que el modelo de clusterizaci칩n las reconozca, utilizando m칠todos como one-hot-encoder, get_dummies o asignaci칩n de valores num칠ricos basados en jerarqu칤as (ej. primaria = 1, secundaria = 2, universidad = 3).

Reemplazamos las cadenas de texto en el dataset con los valores num칠ricos asignados.

Tip: No es necesario codificar todas las columnas categ칩ricas, solo aquellas relevantes para la clusterizaci칩n.

Seleccionamos las variables m치s relevantes para el an치lisis, con el objetivo de agrupar a los clientes en diversos cl칰steres para entender sus caracter칤sticas y brindarles mejor servicio.

Con al menos 6 y m치ximo 12 atributos seleccionados, estandarizamos los datos (que ahora son todos num칠ricos) utilizando StandardScaler, para asegurar que todas las variables est칠n en la misma escala y el modelo aprenda correctamente de todos los atributos. Almacenamos los valores estandarizados en una variable llamada X_std.

Al finalizar, obtenemos un numpy array listo para avanzar a la pr칩xima fase.

### 5. Clusterizaci칩n y validaci칩n

**Clusterizaci칩n**

Utilizamos el algoritmo KMeans para la clusterizaci칩n, aunque se pueden usar otros como Mean Shift o DBSCAN. El objetivo es encontrar el mejor n칰mero de cl칰steres.

**Validaci칩n:**

N칰mero de cl칰steres: Instanciamos de 3 a 10 cl칰steres utilizando X_std. Evaluamos con m칠tricas como Silhouette (m칤nimo 0.50), Davies-Bouldin (m치ximo 0.75) y Calinski-Harabasz (lo m치s alto posible) para determinar la mejor configuraci칩n.

* **Estructura:** Generamos una baseline con n칰meros aleatorios (random_data) y repetimos el paso 2 para compararlo con X_std. Nos aseguramos de que X_std tenga un desempe침o significativamente superior al de random_data.

* **Estabilidad:** Evaluamos la estabilidad segmentando X_std en 3 o 5 partes iguales utilizando array_split() de numpy. Repetimos los pasos de validaci칩n para cada segmento y aseguramos que las variaciones en las m칠tricas no sean mayores a 췀5% entre los sets, garantizando homogeneidad en los cl칰steres.

Si no se logran los resultados sugeridos, revisamos y ajustamos las variables, y repetimos los pasos anteriores.

**Instanciaci칩n de la mejor configuraci칩n de cl칰steres:**

Instanciamos el algoritmo de clusterizaci칩n con la mejor configuraci칩n encontrada y creamos un nuevo atributo en el dataset datos_raw llamado 'cluster' para almacenar las etiquetas de los cl칰steres. No ejecutamos KMeans nuevamente para mantener la consistencia de los cl칰steres.

Realizamos gr치ficos de dispersi칩n comparando variables y a침adiendo una tercera dimensi칩n con los cl칰steres en el par치metro hue. Describimos las observaciones, por ejemplo: "En el cl칰ster 0, de color rojo, se agrupan los clientes que gastan m치s dinero en productos no comestibles". Repetimos hasta obtener descripciones detalladas de cada cl칰ster.

### 6. Descripci칩n de los clusters

Generamos una celda de texto con el resultado consolidado del an치lisis de los cl칰steres. Este an치lisis incluye las caracter칤sticas principales de cada cl칰ster basadas en las observaciones realizadas en los gr치ficos de dispersi칩n.
